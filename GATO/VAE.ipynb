{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.data import get_data\n",
    "from utils.train_val_test import setup_seed\n",
    "import pandas as pd\n",
    "\n",
    "SEED = setup_seed()\n",
    "\n",
    "class Omics(Dataset):\n",
    "    def __init__(self, fold_path, metabric_path, mode=[\"CNA\", \"RNA\", \"CLI\"]):\n",
    "        # Get pre-processed data\n",
    "        omics = get_data(fold_path, metabric_path)\n",
    "\n",
    "        self.mode = mode\n",
    "        \n",
    "        rna = torch.tensor(omics[\"rnanp\"], dtype=torch.float)\n",
    "        cna = torch.tensor(omics[\"cnanp\"], dtype=torch.float)\n",
    "        cli = torch.tensor(omics[\"clin\"], dtype=torch.float)\n",
    "\n",
    "        self.omics_values = {}\n",
    "        self.omics_values[\"CNA\"] = cna\n",
    "        self.omics_values[\"RNA\"] = rna\n",
    "        self.omics_values[\"CLI\"] = cli\n",
    "\n",
    "        self.pam50 = torch.tensor(omics[\"pam50np\"], dtype=torch.int)\n",
    "        self.pam50_labels = omics[\"pam50\"]\n",
    "\n",
    "    def get_omics_data(self, omics_name):\n",
    "        return self.omics_values[omics_name]\n",
    "        \n",
    "    def get_input_dims(self, omics_name):\n",
    "        return self.omics_values[omics_name].size()[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pam50)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.omics_values[omics_name][idx] for omics_name in self.mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Literal\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch_geometric.logging import log\n",
    "from utils.train_val_test import Early_Stopping\n",
    "from abc import ABC, abstractmethod\n",
    "from networks.losses import compute_vae_loss\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "_REGULARISATION = Literal[\"mmd\", \"kld\"]\n",
    "\n",
    "\n",
    "class FC_layer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        output_dim,\n",
    "        normalization: bool = True,\n",
    "        d_p=0,\n",
    "        activation_layer=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Construct a fully-connected block\n",
    "\n",
    "        Parameters:\n",
    "            input_dim (int)         -- the dimension of the input tensor\n",
    "            output_dim (int)        -- the dimension of the output tensor\n",
    "            normalization (bool)    -- need normalization or not\n",
    "            dropout_p (float)       -- probability of an element to be zeroed in a dropout layer\n",
    "            activation_layer (nn)   -- activation function to in the FC block\n",
    "\n",
    "        source @https://github.com/zhangxiaoyu11/OmiEmbed\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_block = [nn.Linear(input_dim, output_dim)]\n",
    "\n",
    "        if normalization:\n",
    "            self.fc_block.append(nn.BatchNorm1d(output_dim))\n",
    "\n",
    "        if 0 < d_p <= 1:\n",
    "            self.fc_block.append(nn.Dropout(p=d_p))\n",
    "\n",
    "        if activation_layer is not None:\n",
    "            self.fc_block.append(activation_layer)\n",
    "\n",
    "        self.fc_block = nn.Sequential(*self.fc_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.fc_block(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "_MODES = Literal[\"CNA\", \"RNA\", \"CLI\"]\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        omics_index=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.beta = params.beta\n",
    "        self.regularisation = params.regularisation\n",
    "        self.loss_fn = params.loss_fn\n",
    "        self.omics_index = omics_index\n",
    "\n",
    "        self.encoder_dense = FC_layer(\n",
    "            params.input_dim,\n",
    "            params.dense_dim,\n",
    "            params.normalization,\n",
    "            params.d_p,\n",
    "            params.activation_fn,\n",
    "        )\n",
    "\n",
    "        self.encoder_mean = nn.Linear(params.dense_dim, params.latent_dim)\n",
    "        self.encoder_log_var = nn.Linear(params.dense_dim, params.latent_dim)\n",
    "\n",
    "        self.decoder_dense = FC_layer(\n",
    "            params.latent_dim,\n",
    "            params.dense_dim,\n",
    "            params.normalization,\n",
    "            params.d_p,\n",
    "            params.activation_fn,\n",
    "        )\n",
    "        self.decoder_output = FC_layer(\n",
    "            params.dense_dim,\n",
    "            params.input_dim,\n",
    "            False,\n",
    "            0,\n",
    "            params.output_activation_fn,\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mean)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.omics_index is not None:\n",
    "            x = x[self.omics_index]\n",
    "        \n",
    "        original_x = x.to(DEVICE)\n",
    "\n",
    "        x = self.encoder_dense(original_x)\n",
    "\n",
    "        latent_mean = self.encoder_mean(x)\n",
    "        latent_log_var = self.encoder_log_var(x)\n",
    "\n",
    "        z = self.reparameterize(latent_mean, latent_log_var)\n",
    "\n",
    "        reconstructed_x = self.decoder_dense(z)\n",
    "        reconstructed_x = self.decoder_output(reconstructed_x)\n",
    "\n",
    "        return original_x, reconstructed_x, latent_mean, latent_log_var, z\n",
    "\n",
    "    def train_loop(self, dataloader, optimizer, epochs):\n",
    "        self.train()\n",
    "\n",
    "        for epoch in range(0, epochs):\n",
    "            loss_sum = 0.0\n",
    "\n",
    "            for batch_idx, x in enumerate(dataloader):\n",
    "                (\n",
    "                    original_x,\n",
    "                    reconstructed_x,\n",
    "                    latent_mean,\n",
    "                    latent_log_var,\n",
    "                    z,\n",
    "                ) = self.forward(x)\n",
    "                loss = compute_vae_loss(\n",
    "                    self.loss_fn,\n",
    "                    self.regularisation,\n",
    "                    self.beta,\n",
    "                    original_x,\n",
    "                    reconstructed_x,\n",
    "                    latent_mean,\n",
    "                    latent_log_var,\n",
    "                )\n",
    "\n",
    "                loss_sum += loss.item()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss = loss_sum / len(dataloader)\n",
    "\n",
    "            val_loss = self.validate(dataloader)\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                log(\n",
    "                    Epoch=epoch,\n",
    "                    Train=train_loss,\n",
    "                    Val=val_loss,\n",
    "                )\n",
    "\n",
    "            # if self.early_stopping is not None and self.early_stopping.check(val_loss):\n",
    "            #     print(\n",
    "            #         f\"Early stopped at epoch: {epoch}, Best Val: {self.early_stopping.best_loss:.4f}\"\n",
    "            #     )\n",
    "\n",
    "            #     # torch.save(model.state_dict(), \"rnaVAE.pth\")\n",
    "            #     break\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, dataloader):\n",
    "        self.eval()\n",
    "        loss_sum = 0.0\n",
    "\n",
    "        for batch_idx, x in enumerate(dataloader):\n",
    "            original_x, reconstructed_x, latent_mean, latent_log_var, z = self.forward(\n",
    "                x\n",
    "            )\n",
    "            loss = compute_vae_loss(\n",
    "                self.loss_fn,\n",
    "                self.regularisation,\n",
    "                self.beta,\n",
    "                original_x,\n",
    "                reconstructed_x,\n",
    "                latent_mean,\n",
    "                latent_log_var,\n",
    "            )\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "        return loss_sum / len(dataloader)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_latent_space(self, dataloader):\n",
    "        self.eval()\n",
    "        latent_space = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, x in enumerate(dataloader):\n",
    "                return_values = self.forward(x)\n",
    "                z = return_values[-1]\n",
    "                if latent_space is not None:\n",
    "                    latent_space = torch.cat((latent_space, z), dim=0)\n",
    "                else:\n",
    "                    latent_space = z\n",
    "\n",
    "        return latent_space.cpu().numpy()\n",
    "\n",
    "\n",
    "class Params_VAE:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        dense_dim,\n",
    "        latent_dim,\n",
    "        lr=0.001,\n",
    "        batch_size=64,\n",
    "        epochs=150,\n",
    "        loss_fn=nn.MSELoss(),\n",
    "        normalization=True,\n",
    "        d_p=0.2,\n",
    "        activation_fn=nn.ELU(),\n",
    "        output_activation_fn=None,\n",
    "        beta=50,\n",
    "        regularisation: _REGULARISATION = \"mmd\",\n",
    "    ):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_fn = loss_fn\n",
    "        self.input_dim = input_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.normalization = normalization\n",
    "        self.d_p = d_p\n",
    "        self.activation_fn = activation_fn\n",
    "        self.beta = beta\n",
    "        self.regularisation = regularisation\n",
    "        self.output_activation_fn = output_activation_fn\n",
    "\n",
    "\n",
    "class H_VAE(VAE):\n",
    "    def __init__(self, input_VAEs, params, early_stopping=None):\n",
    "        super().__init__(params, early_stopping=early_stopping)\n",
    "\n",
    "        self.input_VAEs = input_VAEs\n",
    "        self.params = params\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_omics = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(x)):\n",
    "                self.input_VAEs[i].eval()\n",
    "                latent_data = self.input_VAEs[i].forward(x)\n",
    "                z = latent_data[-1]\n",
    "                latent_omics.append(z)\n",
    "\n",
    "        latent_x = torch.cat(latent_omics, dim=1)\n",
    "\n",
    "        x, reconstructed, latent_mean, latent_log_var, z = super().forward(latent_x)\n",
    "\n",
    "        return latent_x, reconstructed, latent_mean, latent_log_var, z\n",
    "\n",
    "    def train_loop(self, dataloader, optimizers):\n",
    "        for i in range(len(self.input_VAEs)):\n",
    "            print(f\"Training VAE {i+1}\")\n",
    "            self.input_VAEs[i] = self.input_VAEs[i].to(DEVICE)\n",
    "            self.input_VAEs[i].train_loop(dataloader, optimizers[i], self.params.epochs)\n",
    "\n",
    "        print(\"Training H_VAE\")\n",
    "        super().train_loop(dataloader, optimizers[-1], self.params.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import torch.nn as nn\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Benchmark_Classifier():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classifiers = [\n",
    "            GaussianNB(),\n",
    "            SVC(\n",
    "                C=1.5,\n",
    "                kernel=\"rbf\",\n",
    "                random_state=SEED,\n",
    "                gamma=\"auto\",\n",
    "            ),\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=50,\n",
    "                random_state=SEED,\n",
    "                max_features=0.5,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def train(self, dataloader, gt, model):\n",
    "        latent_train = model.get_latent_space(dataloader)\n",
    "        print(\"Training Classifiers\")\n",
    "\n",
    "        for cls in self.classifiers:\n",
    "            cls.fit(latent_train, gt)\n",
    "\n",
    "        return self.evaluate(dataloader, gt, model)\n",
    "\n",
    "    def evaluate(self, dataloader, gt, model):\n",
    "        latent = model.get_latent_space(dataloader)\n",
    "        acc_scores = []\n",
    "        f1_scores = []\n",
    "        for cls in self.classifiers:\n",
    "            predictions = cls.predict(latent)\n",
    "            acc_scores.append(accuracy_score(gt, predictions))\n",
    "            f1_scores.append(f1_score(gt,predictions,average=\"macro\"))\n",
    "\n",
    "        return acc_scores, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FOLD 1 ===\n",
      "--- CLI ---\n",
      "Epoch: 000, Train: 0.6069, Val: 0.5704\n",
      "Epoch: 020, Train: 0.5063, Val: 0.5058\n",
      "Epoch: 040, Train: 0.4933, Val: 0.4955\n",
      "Epoch: 060, Train: 0.4873, Val: 0.4879\n",
      "Epoch: 080, Train: 0.4847, Val: 0.4852\n",
      "Epoch: 100, Train: 0.4818, Val: 0.4829\n",
      "Epoch: 120, Train: 0.4802, Val: 0.4808\n",
      "Epoch: 140, Train: 0.4794, Val: 0.4796\n",
      "--- CNA ---\n",
      "Epoch: 000, Train: 0.6986, Val: 0.6841\n",
      "Epoch: 020, Train: 0.6388, Val: 0.6427\n",
      "Epoch: 040, Train: 0.6344, Val: 0.6397\n",
      "Epoch: 060, Train: 0.6319, Val: 0.6375\n",
      "Epoch: 080, Train: 0.6305, Val: 0.6359\n",
      "Epoch: 100, Train: 0.6292, Val: 0.6333\n",
      "Epoch: 120, Train: 0.6292, Val: 0.6321\n",
      "Epoch: 140, Train: 0.6269, Val: 0.6303\n",
      "--- RNA ---\n",
      "Epoch: 000, Train: 0.2219, Val: 0.1125\n",
      "Epoch: 020, Train: 0.0071, Val: 0.0071\n",
      "Epoch: 040, Train: 0.0056, Val: 0.0058\n",
      "Epoch: 060, Train: 0.0048, Val: 0.0048\n",
      "Epoch: 080, Train: 0.0044, Val: 0.0045\n",
      "Epoch: 100, Train: 0.0040, Val: 0.0041\n",
      "Epoch: 120, Train: 0.0155, Val: 0.0164\n",
      "Epoch: 140, Train: 0.0092, Val: 0.0093\n",
      "=== FOLD 2 ===\n",
      "--- CLI ---\n",
      "Epoch: 000, Train: 0.6067, Val: 0.5687\n",
      "Epoch: 020, Train: 0.5044, Val: 0.5060\n",
      "Epoch: 040, Train: 0.4932, Val: 0.4946\n",
      "Epoch: 060, Train: 0.4874, Val: 0.4883\n",
      "Epoch: 080, Train: 0.4832, Val: 0.4839\n",
      "Epoch: 100, Train: 0.4808, Val: 0.4824\n",
      "Epoch: 120, Train: 0.4790, Val: 0.4800\n",
      "Epoch: 140, Train: 0.4785, Val: 0.4786\n",
      "--- CNA ---\n",
      "Epoch: 000, Train: 0.6984, Val: 0.7062\n",
      "Epoch: 020, Train: 0.6386, Val: 0.6435\n",
      "Epoch: 040, Train: 0.6351, Val: 0.6387\n",
      "Epoch: 060, Train: 0.6328, Val: 0.6376\n",
      "Epoch: 080, Train: 0.6307, Val: 0.6373\n",
      "Epoch: 100, Train: 0.6303, Val: 0.6343\n",
      "Epoch: 120, Train: 0.6291, Val: 0.6311\n",
      "Epoch: 140, Train: 0.6275, Val: 0.6296\n",
      "--- RNA ---\n",
      "Epoch: 000, Train: 0.2244, Val: 0.0943\n",
      "Epoch: 020, Train: 0.0065, Val: 0.0066\n",
      "Epoch: 040, Train: 0.0052, Val: 0.0052\n",
      "Epoch: 060, Train: 0.0045, Val: 0.0045\n",
      "Epoch: 080, Train: 0.0044, Val: 0.0045\n",
      "Epoch: 100, Train: 0.0176, Val: 0.0221\n",
      "Epoch: 120, Train: 0.0138, Val: 0.0184\n",
      "Epoch: 140, Train: 0.0125, Val: 0.0164\n",
      "=== FOLD 3 ===\n",
      "--- CLI ---\n",
      "Epoch: 000, Train: 0.6047, Val: 0.5701\n",
      "Epoch: 020, Train: 0.5060, Val: 0.5056\n",
      "Epoch: 040, Train: 0.4949, Val: 0.4949\n",
      "Epoch: 060, Train: 0.4896, Val: 0.4907\n",
      "Epoch: 080, Train: 0.4846, Val: 0.4859\n",
      "Epoch: 100, Train: 0.4822, Val: 0.4832\n",
      "Epoch: 120, Train: 0.4806, Val: 0.4804\n",
      "Epoch: 140, Train: 0.4792, Val: 0.4800\n",
      "--- CNA ---\n",
      "Epoch: 000, Train: 0.6976, Val: 0.7026\n",
      "Epoch: 020, Train: 0.6388, Val: 0.6424\n",
      "Epoch: 040, Train: 0.6347, Val: 0.6402\n",
      "Epoch: 060, Train: 0.6322, Val: 0.6373\n",
      "Epoch: 080, Train: 0.6312, Val: 0.6348\n",
      "Epoch: 100, Train: 0.6301, Val: 0.6353\n",
      "Epoch: 120, Train: 0.6299, Val: 0.6332\n",
      "Epoch: 140, Train: 0.6299, Val: 0.6340\n",
      "--- RNA ---\n",
      "Epoch: 000, Train: 0.2215, Val: 0.1314\n",
      "Epoch: 020, Train: 0.0068, Val: 0.0069\n",
      "Epoch: 040, Train: 0.0053, Val: 0.0054\n",
      "Epoch: 060, Train: 0.0047, Val: 0.0049\n",
      "Epoch: 080, Train: 0.0044, Val: 0.0043\n",
      "Epoch: 100, Train: 0.0039, Val: 0.0041\n",
      "Epoch: 120, Train: 0.0122, Val: 0.0137\n",
      "Epoch: 140, Train: 0.0078, Val: 0.0080\n",
      "=== FOLD 4 ===\n",
      "--- CLI ---\n",
      "Epoch: 000, Train: 0.6069, Val: 0.5694\n",
      "Epoch: 020, Train: 0.5048, Val: 0.5055\n",
      "Epoch: 040, Train: 0.4938, Val: 0.4954\n",
      "Epoch: 060, Train: 0.4886, Val: 0.4891\n",
      "Epoch: 080, Train: 0.4844, Val: 0.4850\n",
      "Epoch: 100, Train: 0.4814, Val: 0.4822\n",
      "Epoch: 120, Train: 0.4795, Val: 0.4792\n",
      "Epoch: 140, Train: 0.4783, Val: 0.4789\n",
      "--- CNA ---\n",
      "Epoch: 000, Train: 0.6985, Val: 0.6747\n",
      "Epoch: 020, Train: 0.6387, Val: 0.6422\n",
      "Epoch: 040, Train: 0.6342, Val: 0.6402\n",
      "Epoch: 060, Train: 0.6322, Val: 0.6389\n",
      "Epoch: 080, Train: 0.6305, Val: 0.6361\n",
      "Epoch: 100, Train: 0.6294, Val: 0.6335\n",
      "Epoch: 120, Train: 0.6287, Val: 0.6342\n",
      "Epoch: 140, Train: 0.6275, Val: 0.6328\n",
      "--- RNA ---\n",
      "Epoch: 000, Train: 0.2209, Val: 0.1645\n",
      "Epoch: 020, Train: 0.0070, Val: 0.0070\n",
      "Epoch: 040, Train: 0.0054, Val: 0.0054\n",
      "Epoch: 060, Train: 0.0048, Val: 0.0048\n",
      "Epoch: 080, Train: 0.0043, Val: 0.0044\n",
      "Epoch: 100, Train: 0.0040, Val: 0.0042\n",
      "Epoch: 120, Train: 0.0039, Val: 0.0041\n",
      "Epoch: 140, Train: 0.0036, Val: 0.0037\n",
      "=== FOLD 5 ===\n",
      "--- CLI ---\n",
      "Epoch: 000, Train: 0.6065, Val: 0.5681\n",
      "Epoch: 020, Train: 0.5073, Val: 0.5071\n",
      "Epoch: 040, Train: 0.4935, Val: 0.4957\n",
      "Epoch: 060, Train: 0.4882, Val: 0.4890\n",
      "Epoch: 080, Train: 0.4840, Val: 0.4845\n",
      "Epoch: 100, Train: 0.4812, Val: 0.4824\n",
      "Epoch: 120, Train: 0.4797, Val: 0.4806\n",
      "Epoch: 140, Train: 0.4787, Val: 0.4787\n",
      "--- CNA ---\n",
      "Epoch: 000, Train: 0.6982, Val: 0.7049\n",
      "Epoch: 020, Train: 0.6382, Val: 0.6419\n",
      "Epoch: 040, Train: 0.6343, Val: 0.6377\n",
      "Epoch: 060, Train: 0.6318, Val: 0.6344\n",
      "Epoch: 080, Train: 0.6303, Val: 0.6349\n",
      "Epoch: 100, Train: 0.6287, Val: 0.6327\n",
      "Epoch: 120, Train: 0.6281, Val: 0.6325\n",
      "Epoch: 140, Train: 0.6291, Val: 0.6306\n",
      "--- RNA ---\n",
      "Epoch: 000, Train: 0.2232, Val: 0.0954\n",
      "Epoch: 020, Train: 0.0070, Val: 0.0070\n",
      "Epoch: 040, Train: 0.0053, Val: 0.0054\n",
      "Epoch: 060, Train: 0.0048, Val: 0.0047\n",
      "Epoch: 080, Train: 0.0044, Val: 0.0044\n",
      "Epoch: 100, Train: 0.0040, Val: 0.0040\n",
      "Epoch: 120, Train: 0.0165, Val: 0.0188\n",
      "Epoch: 140, Train: 0.0155, Val: 0.0177\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from utils.train_val_test import Early_Stopping\n",
    "import time\n",
    "\n",
    "metabric_path = \"data/MBdata_33CLINwMiss_1KfGE_1KfCNA.csv\"\n",
    "\n",
    "EPOCHS = 150\n",
    "N_FOLDS = 5\n",
    "fold_dir = \"data/5-fold_pam50stratified/\"\n",
    "file_name = \"MBdata_33CLINwMiss_1KfGE_1KfCNA\"\n",
    "\n",
    "vae_params = Params_VAE(None, 256, 256 // 2, epochs=EPOCHS, regularisation=\"mmd\")\n",
    "\n",
    "omics_types = [\"CLI\", \"CNA\", \"RNA\"]\n",
    "\n",
    "save_dir = os.path.join(\"results\", f'{time.strftime(\"%m%d%H%M%S\", time.gmtime())}')\n",
    "\n",
    "# accTrain_list = []\n",
    "# accTest_list = []\n",
    "\n",
    "for k in range(1, N_FOLDS + 1):\n",
    "    # metrics_list = []\n",
    "\n",
    "    print(f\"=== FOLD {k} ===\")\n",
    "\n",
    "    train_data_path = os.path.join(fold_dir, f\"fold{k}\", file_name + \"_train.csv\")\n",
    "    test_data_path = os.path.join(fold_dir, f\"fold{k}\", file_name + \"_test.csv\")\n",
    "\n",
    "    train_omics = Omics(train_data_path, metabric_path, mode=omics_types)\n",
    "    test_omics = Omics(test_data_path, metabric_path, mode=omics_types)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_omics, batch_size=vae_params.batch_size, shuffle=False\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_omics, batch_size=vae_params.batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"fold_{k}\")\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "    for i in range(len(omics_types)):\n",
    "        print(f\"--- {omics_types[i]} ---\")\n",
    "        \n",
    "        vae_params.input_dim = train_omics.get_input_dims(omics_types[i])\n",
    "\n",
    "        if omics_types[i] == \"CNA\" or omics_types[i] == \"CLI\":\n",
    "            vae_params.loss_fn = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "        if omics_types[i] == \"RNA\":\n",
    "            vae_params.loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "        vae = VAE(vae_params, omics_index=i)\n",
    "        optimizer = torch.optim.Adam(vae.parameters(), lr=vae_params.lr)\n",
    "        vae.to(DEVICE)\n",
    "        vae.train_loop(train_dataloader, optimizer, vae_params.epochs)\n",
    "\n",
    "        filename = f\"{omics_types[i]}\"\n",
    "        torch.save(vae.state_dict(), os.path.join(save_path, filename + \".pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CLI ---\n",
      "=== FOLD 1 ===\n",
      "Training Classifiers\n",
      "\n",
      "Train Acc: [0.38257575757575757, 0.8939393939393939, 0.94760101010101], Test Acc: [0.32323232323232326, 0.4015151515151515, 0.39141414141414144]\n",
      "\n",
      "--- CLI ---\n",
      "=== FOLD 2 ===\n",
      "Training Classifiers\n",
      "\n",
      "Train Acc: [0.4097222222222222, 0.8977272727272727, 0.9570707070707071], Test Acc: [0.3282828282828283, 0.4116161616161616, 0.398989898989899]\n",
      "\n",
      "--- CLI ---\n",
      "=== FOLD 3 ===\n",
      "Training Classifiers\n",
      "\n",
      "Train Acc: [0.39330808080808083, 0.8920454545454546, 0.9494949494949495], Test Acc: [0.3787878787878788, 0.39646464646464646, 0.40404040404040403]\n",
      "\n",
      "--- CLI ---\n",
      "=== FOLD 4 ===\n",
      "Training Classifiers\n",
      "\n",
      "Train Acc: [0.4078282828282828, 0.8939393939393939, 0.952020202020202], Test Acc: [0.3181818181818182, 0.35353535353535354, 0.3712121212121212]\n",
      "\n",
      "--- CLI ---\n",
      "=== FOLD 5 ===\n",
      "Training Classifiers\n",
      "\n",
      "Train Acc: [0.3996212121212121, 0.8945707070707071, 0.9564393939393939], Test Acc: [0.3106060606060606, 0.3787878787878788, 0.3686868686868687]\n",
      "\n",
      "--- CNA ---\n",
      "=== FOLD 1 ===\n",
      "Training Classifiers\n",
      "\n",
      "Train Acc: [0.4880050505050505, 0.865530303030303, 0.9659090909090909], Test Acc: [0.5025252525252525, 0.553030303030303, 0.5252525252525253]\n",
      "\n",
      "--- CNA ---\n",
      "=== FOLD 2 ===\n",
      "Training Classifiers\n",
      "\n",
      "Train Acc: [0.4602272727272727, 0.8497474747474747, 0.9652777777777778], Test Acc: [0.43434343434343436, 0.5328282828282829, 0.5681818181818182]\n",
      "\n",
      "--- CNA ---\n",
      "=== FOLD 3 ===\n",
      "Training Classifiers\n",
      "\n",
      "Train Acc: [0.514520202020202, 0.851010101010101, 0.9627525252525253], Test Acc: [0.46464646464646464, 0.5808080808080808, 0.5404040404040404]\n",
      "\n",
      "--- CNA ---\n",
      "=== FOLD 4 ===\n",
      "Training Classifiers\n",
      "\n",
      "Train Acc: [0.4564393939393939, 0.8585858585858586, 0.9640151515151515], Test Acc: [0.41919191919191917, 0.5126262626262627, 0.5151515151515151]\n",
      "\n",
      "--- CNA ---\n",
      "=== FOLD 5 ===\n",
      "Training Classifiers\n",
      "\n",
      "Train Acc: [0.51010101010101, 0.8566919191919192, 0.9678030303030303], Test Acc: [0.48737373737373735, 0.5656565656565656, 0.5555555555555556]\n",
      "\n",
      "--- RNA ---\n",
      "=== FOLD 1 ===\n",
      "Training Classifiers\n",
      "\n",
      "Train Acc: [0.6016414141414141, 0.6893939393939394, 0.9993686868686869], Test Acc: [0.6186868686868687, 0.7146464646464646, 0.6641414141414141]\n",
      "\n",
      "--- RNA ---\n",
      "=== FOLD 2 ===\n",
      "Training Classifiers\n",
      "\n",
      "Train Acc: [0.48547979797979796, 0.6799242424242424, 1.0], Test Acc: [0.45707070707070707, 0.601010101010101, 0.5782828282828283]\n",
      "\n",
      "--- RNA ---\n",
      "=== FOLD 3 ===\n",
      "Training Classifiers\n",
      "\n",
      "Train Acc: [0.615530303030303, 0.7215909090909091, 1.0], Test Acc: [0.6035353535353535, 0.696969696969697, 0.6388888888888888]\n",
      "\n",
      "--- RNA ---\n",
      "=== FOLD 4 ===\n",
      "Training Classifiers\n",
      "\n",
      "Train Acc: [0.23547979797979798, 0.5991161616161617, 0.9987373737373737], Test Acc: [0.31313131313131315, 0.5808080808080808, 0.5631313131313131]\n",
      "\n",
      "--- RNA ---\n",
      "=== FOLD 5 ===\n",
      "Training Classifiers\n",
      "\n",
      "Train Acc: [0.4078282828282828, 0.6540404040404041, 1.0], Test Acc: [0.3484848484848485, 0.6186868686868687, 0.6262626262626263]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "metabric_path = \"data/MBdata_33CLINwMiss_1KfGE_1KfCNA.csv\"\n",
    "\n",
    "N_FOLDS = 5\n",
    "fold_dir = \"data/5-fold_pam50stratified/\"\n",
    "file_name = \"MBdata_33CLINwMiss_1KfGE_1KfCNA\"\n",
    "\n",
    "vae_params = Params_VAE(None, 256, 256 // 2)\n",
    "\n",
    "omics_types = [\"CLI\", \"CNA\", \"RNA\"]\n",
    "\n",
    "load_dir = \"results/0110183803\"\n",
    "\n",
    "metrics = []\n",
    "f1_scores = {}\n",
    "\n",
    "for i in range(len(omics_types)):\n",
    "    print(f\"--- {omics_types[i]} ---\")\n",
    "    acc_scores = []\n",
    "\n",
    "    for k in range(1, N_FOLDS + 1):\n",
    "        print(f\"=== FOLD {k} ===\")\n",
    "\n",
    "        train_data_path = os.path.join(fold_dir, f\"fold{k}\", file_name + \"_train.csv\")\n",
    "        test_data_path = os.path.join(fold_dir, f\"fold{k}\", file_name + \"_test.csv\")\n",
    "\n",
    "        train_omics = Omics(train_data_path, metabric_path, mode=omics_types)\n",
    "        test_omics = Omics(test_data_path, metabric_path, mode=omics_types)\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_omics, batch_size=vae_params.batch_size, shuffle=False\n",
    "        )\n",
    "        test_dataloader = DataLoader(\n",
    "            test_omics, batch_size=vae_params.batch_size, shuffle=False\n",
    "        )\n",
    "\n",
    "        load_path = os.path.join(load_dir, f\"fold_{k}\")\n",
    "\n",
    "        vae_params.input_dim = train_omics.get_input_dims(omics_types[i])\n",
    "\n",
    "        model = VAE(\n",
    "            vae_params, omics_index=i\n",
    "        )  # we do not specify ``weights``, i.e. create untrained model\n",
    "        model.load_state_dict(\n",
    "            torch.load(os.path.join(load_path, omics_types[i] + \".pth\"))\n",
    "        )\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        classifier = Benchmark_Classifier()\n",
    "        accTrain, f1Train = classifier.train(train_dataloader, train_omics.pam50, model)\n",
    "        accTest, f1Test = classifier.evaluate(test_dataloader, test_omics.pam50, model)\n",
    "\n",
    "        print(f\"\\nTrain Acc: {accTrain}, Test Acc: {accTest}\\n\")\n",
    "\n",
    "        acc_scores.append([*accTest, *f1Test])\n",
    "    metrics.append(np.array(acc_scores).mean(axis=0))\n",
    "\n",
    "data = [[omics_types[i], *metrics[i]] for i in range(len(omics_types))]\n",
    "columns = [\"Omics\", \"Acc_NB\", \"Acc_SVM\", \"Acc_RF\", \"f1_NB\", \"f1_SVM\", \"f1_RF\"]\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.to_csv(os.path.join(load_dir, \"VAE_metrics.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_latent_values(model, dataloader):\n",
    "    model.eval()\n",
    "    latent_space = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, x in enumerate(dataloader):\n",
    "            return_values = model.forward(x)\n",
    "            z = return_values[-1]\n",
    "            if latent_space is not None:\n",
    "                latent_space = torch.cat((latent_space, z), dim=0)\n",
    "            else:\n",
    "                latent_space = z\n",
    "    return latent_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FOLD 1 ===\n",
      "--- CNA ---\n",
      "--- RNA ---\n",
      "torch.Size([1584, 256])\n",
      "torch.Size([396, 256])\n",
      "Epoch: 000, Train: 1.2176, Val: 0.8204\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VAE' object has no attribute 'early_stopping'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(h_vae\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mh_vae_params\u001b[38;5;241m.\u001b[39mlr)\n\u001b[1;32m     73\u001b[0m h_vae\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 75\u001b[0m \u001b[43mh_vae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH_train_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_vae_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH_VAE_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(omics_types)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(vae\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(load_path, filename \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[19], line 165\u001b[0m, in \u001b[0;36mVAE.train_loop\u001b[0;34m(self, dataloader, optimizer, epochs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    159\u001b[0m     log(\n\u001b[1;32m    160\u001b[0m         Epoch\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m    161\u001b[0m         Train\u001b[38;5;241m=\u001b[39mtrain_loss,\n\u001b[1;32m    162\u001b[0m         Val\u001b[38;5;241m=\u001b[39mval_loss,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping\u001b[38;5;241m.\u001b[39mcheck(val_loss):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEarly stopped at epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Best Val: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping\u001b[38;5;241m.\u001b[39mbest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# torch.save(model.state_dict(), \"rnaVAE.pth\")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/omics/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VAE' object has no attribute 'early_stopping'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "metabric_path = \"data/MBdata_33CLINwMiss_1KfGE_1KfCNA.csv\"\n",
    "\n",
    "N_FOLDS = 5\n",
    "fold_dir = \"data/5-fold_pam50stratified/\"\n",
    "file_name = \"MBdata_33CLINwMiss_1KfGE_1KfCNA\"\n",
    "\n",
    "h_vae_params = Params_VAE(256, 256, 64, epochs=150)\n",
    "vae_params = Params_VAE(None, 256, 256 // 2)\n",
    "\n",
    "omics_types = [\"CNA\", \"RNA\"]\n",
    "\n",
    "load_dir = \"results/0110183803\"\n",
    "\n",
    "metrics = []\n",
    "f1_scores = {}\n",
    "\n",
    "\n",
    "acc_scores = []\n",
    "\n",
    "for k in range(1, N_FOLDS + 1):\n",
    "    print(f\"=== FOLD {k} ===\")\n",
    "\n",
    "    train_data_path = os.path.join(fold_dir, f\"fold{k}\", file_name + \"_train.csv\")\n",
    "    test_data_path = os.path.join(fold_dir, f\"fold{k}\", file_name + \"_test.csv\")\n",
    "\n",
    "    train_omics = Omics(train_data_path, metabric_path, mode=omics_types)\n",
    "    test_omics = Omics(test_data_path, metabric_path, mode=omics_types)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_omics, batch_size=vae_params.batch_size, shuffle=False\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_omics, batch_size=vae_params.batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    load_path = os.path.join(load_dir, f\"fold_{k}\")\n",
    "    train_latents = []\n",
    "    test_latents = []\n",
    "\n",
    "    for i in range(len(omics_types)):\n",
    "        print(f\"--- {omics_types[i]} ---\")\n",
    "        vae_params.input_dim = train_omics.get_input_dims(omics_types[i])\n",
    "\n",
    "        vae = VAE(\n",
    "            vae_params, omics_index=i\n",
    "        )\n",
    "        vae.load_state_dict(\n",
    "            torch.load(os.path.join(load_path, omics_types[i] + \".pth\"))\n",
    "        )\n",
    "        vae.to(DEVICE)\n",
    "        \n",
    "        train_latents.append(get_latent_values(vae, train_dataloader))\n",
    "        test_latents.append(get_latent_values(vae, test_dataloader))\n",
    "\n",
    "    train_input = torch.cat(train_latents, dim=1)\n",
    "    print(train_input.size())\n",
    "    H_train_dataloader = DataLoader(\n",
    "        train_input, batch_size=h_vae_params.batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    test_input = torch.cat(test_latents, dim=1)\n",
    "    print(test_input.size())\n",
    "    H_test_dataloader = DataLoader(\n",
    "        test_input, batch_size=h_vae_params.batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    h_vae = VAE(\n",
    "        h_vae_params\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(h_vae.parameters(), lr=h_vae_params.lr)\n",
    "    h_vae.to(DEVICE)\n",
    "    \n",
    "    h_vae.train_loop(H_train_dataloader, optimizer, h_vae_params.epochs)\n",
    "\n",
    "    filename = f\"H_VAE_{'_'.join(omics_types)}\"\n",
    "    torch.save(vae.state_dict(), os.path.join(load_path, filename + \".pth\"))\n",
    "    \n",
    "    classifier = Benchmark_Classifier()\n",
    "    accTrain, f1Train = classifier.train(H_train_dataloader, train_omics.pam50, h_vae)\n",
    "    accTest, f1Test = classifier.evaluate(H_test_dataloader, test_omics.pam50, h_vae)\n",
    "\n",
    "    print(f\"\\nTrain Acc: {accTrain}, Test Acc: {accTest}\\n\")\n",
    "\n",
    "    acc_scores.append([*accTest, *f1Test])\n",
    "\n",
    "metrics = np.array(acc_scores).mean(axis=0)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Omics</th>\n",
       "      <th>Acc_NB</th>\n",
       "      <th>Acc_SVM</th>\n",
       "      <th>Acc_RF</th>\n",
       "      <th>f1_NB</th>\n",
       "      <th>f1_SVM</th>\n",
       "      <th>f1_RF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CLI</td>\n",
       "      <td>0.331818</td>\n",
       "      <td>0.388384</td>\n",
       "      <td>0.386869</td>\n",
       "      <td>0.210578</td>\n",
       "      <td>0.209724</td>\n",
       "      <td>0.191306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNA</td>\n",
       "      <td>0.461616</td>\n",
       "      <td>0.548990</td>\n",
       "      <td>0.540909</td>\n",
       "      <td>0.377095</td>\n",
       "      <td>0.407521</td>\n",
       "      <td>0.397174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RNA</td>\n",
       "      <td>0.468182</td>\n",
       "      <td>0.642424</td>\n",
       "      <td>0.614141</td>\n",
       "      <td>0.375507</td>\n",
       "      <td>0.483145</td>\n",
       "      <td>0.471162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H_VAE</td>\n",
       "      <td>0.501010</td>\n",
       "      <td>0.604040</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.401407</td>\n",
       "      <td>0.454947</td>\n",
       "      <td>0.426509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Omics    Acc_NB   Acc_SVM    Acc_RF     f1_NB    f1_SVM     f1_RF\n",
       "0    CLI  0.331818  0.388384  0.386869  0.210578  0.209724  0.191306\n",
       "1    CNA  0.461616  0.548990  0.540909  0.377095  0.407521  0.397174\n",
       "2    RNA  0.468182  0.642424  0.614141  0.375507  0.483145  0.471162\n",
       "3  H_VAE  0.501010  0.604040  0.575758  0.401407  0.454947  0.426509"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(load_dir, \"VAE_metrics.csv\"), index_col=0)\n",
    "df.loc[len(df.index)] = [\"H_VAE\", *metrics]\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
