{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.logging import log\n",
    "from torch_geometric.utils import to_edge_index\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import random\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "\n",
    "def setup_seed(seed=SEED):\n",
    "    \"\"\"\n",
    "    setup seed to make the experiments deterministic\n",
    "\n",
    "    Parameters:\n",
    "        seed(int) -- the random seed\n",
    "\n",
    "    @source https://github.com/zhangxiaoyu11/OmiEmbed\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "setup_seed(42)\n",
    "\n",
    "def get_edge_index(X, threshold=0.005):\n",
    "    if threshold:\n",
    "        X[X < threshold] = 0\n",
    "\n",
    "    return to_edge_index((torch.tensor(X, dtype=torch.float).to_sparse()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latent = pd.read_csv(\"./data/MoGCN_results/latent_data.csv\")\n",
    "x = df_latent.iloc[:, 1:].values\n",
    "\n",
    "df_results = pd.read_csv(\"./data/TCGA/sample_classes.csv\")\n",
    "y = df_results[\"class\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LumA', 'LumB', 'Basal', 'Her2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = df_results[[\"PAM50Call_RNAseq\",\"class\"]].value_counts()\n",
    "counts = counts.reset_index().sort_values([\"class\"])\n",
    "labels = counts[\"PAM50Call_RNAseq\"].to_list()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snf = pd.read_csv(\"./data/MoGCN_results/SNF_fused_matrix.csv\")\n",
    "adj = df_snf.iloc[:,1:].values\n",
    "np.fill_diagonal(adj, 0)\n",
    "edge_index, edge_attr = get_edge_index(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([866])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test = pd.read_csv(\"./data/TCGA/test_sample.csv\")\n",
    "test_mask = np.array(df_latent[\"Sample\"].isin(list_test[\"Sample_ID\"]))\n",
    "\n",
    "list_train = pd.read_csv(\"./data/TCGA/train_sample.csv\")\n",
    "train_mask = np.array(df_latent[\"Sample\"].isin(list_train[\"Sample_ID\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Data(\n",
    "    x=torch.tensor(x, dtype=torch.float32),\n",
    "    # edge_index = torch.tensor([[], []], dtype=torch.long),\n",
    "    edge_index=edge_index,\n",
    "    edge_attr=edge_attr,\n",
    "    y=torch.tensor(y, dtype=torch.long),\n",
    ")\n",
    "\n",
    "dataset.train_mask = train_mask\n",
    "dataset.test_mask = test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def train_loop(self, data, optimizer, epochs=100):\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = self.forward(data.x, data.edge_index, data.edge_attr)\n",
    "            loss = F.cross_entropy(pred[train_mask], data.y[train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_acc, train_f1 = self.validate(data, data.train_mask)\n",
    "            val_acc, val_f1 = self.validate(data, data.test_mask)\n",
    "\n",
    "            log(\n",
    "                Epoch=epoch,\n",
    "                Loss=loss,\n",
    "                Train_Acc=train_acc,\n",
    "                Val_Acc=val_acc,\n",
    "                Train_f1=train_f1,\n",
    "                Val_f1=val_f1,\n",
    "            )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, data, mask):\n",
    "        self.eval()\n",
    "        pred = self.forward(data.x, data.edge_index, data.edge_attr)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "\n",
    "        f1 = f1_score(\n",
    "            data.y[mask].cpu().numpy(),\n",
    "            pred[mask].cpu().numpy(),\n",
    "            average=\"macro\",\n",
    "        )\n",
    "        acc = accuracy_score(data.y[mask].cpu().numpy(), pred[mask].cpu().numpy())\n",
    "\n",
    "        return acc, f1\n",
    "\n",
    "\n",
    "class GCN(GNN):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # self.fc = GCNConv(hidden_channels, out_channels)\n",
    "        self.fc = nn.Linear(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc(x)\n",
    "        # x = self.fc(x, edge_index, edge_weight)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GAT(GNN):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels)\n",
    "        self.conv2 = GATv2Conv(hidden_channels, hidden_channels)\n",
    "        self.fc = GATv2Conv(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        # x = self.fc(x)\n",
    "        x = self.fc(x, edge_index, edge_weight)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = dataset.x.shape[1]\n",
    "hidden_dim = 64\n",
    "n_classes = len(labels)\n",
    "dropout = 0.5\n",
    "lr = 0.001\n",
    "wd = 0.01\n",
    "\n",
    "model = GCN(input_dim, hidden_dim, n_classes, dropout)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr, weight_decay=wd)\n",
    "\n",
    "model.to(DEVICE)\n",
    "data = dataset.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.4276748895645142, Train_Acc: 0.4831, Val_Acc: 0.5147, Train_f1: 0.1695, Val_f1: 0.1699\n",
      "Epoch: 002, Loss: 1.3867160081863403, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 003, Loss: 1.3300784826278687, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 004, Loss: 1.2930636405944824, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 005, Loss: 1.2810801267623901, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 006, Loss: 1.247971773147583, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 007, Loss: 1.2436822652816772, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 008, Loss: 1.1955045461654663, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 009, Loss: 1.1963462829589844, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 010, Loss: 1.1959319114685059, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 011, Loss: 1.1942030191421509, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 012, Loss: 1.1727983951568604, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 013, Loss: 1.1569185256958008, Train_Acc: 0.4831, Val_Acc: 0.5147, Train_f1: 0.1681, Val_f1: 0.1699\n",
      "Epoch: 014, Loss: 1.1239451169967651, Train_Acc: 0.4966, Val_Acc: 0.5294, Train_f1: 0.2002, Val_f1: 0.1933\n",
      "Epoch: 015, Loss: 1.1344633102416992, Train_Acc: 0.5260, Val_Acc: 0.5294, Train_f1: 0.2578, Val_f1: 0.1933\n",
      "Epoch: 016, Loss: 1.099442720413208, Train_Acc: 0.5666, Val_Acc: 0.6765, Train_f1: 0.3199, Val_f1: 0.3569\n",
      "Epoch: 017, Loss: 1.0825427770614624, Train_Acc: 0.5982, Val_Acc: 0.6912, Train_f1: 0.3565, Val_f1: 0.3688\n",
      "Epoch: 018, Loss: 1.057599663734436, Train_Acc: 0.6253, Val_Acc: 0.6912, Train_f1: 0.3827, Val_f1: 0.3688\n",
      "Epoch: 019, Loss: 1.0737931728363037, Train_Acc: 0.6456, Val_Acc: 0.7353, Train_f1: 0.3996, Val_f1: 0.4016\n",
      "Epoch: 020, Loss: 1.0600813627243042, Train_Acc: 0.6591, Val_Acc: 0.7647, Train_f1: 0.4085, Val_f1: 0.4214\n",
      "Epoch: 021, Loss: 1.0203157663345337, Train_Acc: 0.6591, Val_Acc: 0.7794, Train_f1: 0.4085, Val_f1: 0.4309\n",
      "Epoch: 022, Loss: 1.003493309020996, Train_Acc: 0.6637, Val_Acc: 0.7794, Train_f1: 0.4123, Val_f1: 0.4309\n",
      "Epoch: 023, Loss: 0.9866703152656555, Train_Acc: 0.6682, Val_Acc: 0.7941, Train_f1: 0.4199, Val_f1: 0.4400\n",
      "Epoch: 024, Loss: 0.972660481929779, Train_Acc: 0.6704, Val_Acc: 0.8235, Train_f1: 0.4218, Val_f1: 0.4576\n",
      "Epoch: 025, Loss: 0.9643055200576782, Train_Acc: 0.6772, Val_Acc: 0.8235, Train_f1: 0.4311, Val_f1: 0.4576\n",
      "Epoch: 026, Loss: 0.9244688749313354, Train_Acc: 0.6772, Val_Acc: 0.8235, Train_f1: 0.4302, Val_f1: 0.4576\n",
      "Epoch: 027, Loss: 0.9152061343193054, Train_Acc: 0.6817, Val_Acc: 0.8235, Train_f1: 0.4422, Val_f1: 0.4576\n",
      "Epoch: 028, Loss: 0.9024888277053833, Train_Acc: 0.6862, Val_Acc: 0.8235, Train_f1: 0.4487, Val_f1: 0.4576\n",
      "Epoch: 029, Loss: 0.8849443793296814, Train_Acc: 0.6907, Val_Acc: 0.8235, Train_f1: 0.4542, Val_f1: 0.4547\n",
      "Epoch: 030, Loss: 0.8637698888778687, Train_Acc: 0.6907, Val_Acc: 0.8235, Train_f1: 0.4542, Val_f1: 0.4547\n",
      "Epoch: 031, Loss: 0.8396995067596436, Train_Acc: 0.6975, Val_Acc: 0.8235, Train_f1: 0.4772, Val_f1: 0.4547\n",
      "Epoch: 032, Loss: 0.8673526048660278, Train_Acc: 0.7043, Val_Acc: 0.8382, Train_f1: 0.4928, Val_f1: 0.4632\n",
      "Epoch: 033, Loss: 0.8317220211029053, Train_Acc: 0.7065, Val_Acc: 0.8382, Train_f1: 0.4995, Val_f1: 0.4632\n",
      "Epoch: 034, Loss: 0.8175587058067322, Train_Acc: 0.7156, Val_Acc: 0.8382, Train_f1: 0.5235, Val_f1: 0.4632\n",
      "Epoch: 035, Loss: 0.7898575663566589, Train_Acc: 0.7178, Val_Acc: 0.8529, Train_f1: 0.5306, Val_f1: 0.5910\n",
      "Epoch: 036, Loss: 0.7457457780838013, Train_Acc: 0.7223, Val_Acc: 0.8529, Train_f1: 0.5470, Val_f1: 0.5910\n",
      "Epoch: 037, Loss: 0.7437379956245422, Train_Acc: 0.7381, Val_Acc: 0.8529, Train_f1: 0.5852, Val_f1: 0.5885\n",
      "Epoch: 038, Loss: 0.7206903100013733, Train_Acc: 0.7449, Val_Acc: 0.8676, Train_f1: 0.6039, Val_f1: 0.6688\n",
      "Epoch: 039, Loss: 0.7303771376609802, Train_Acc: 0.7630, Val_Acc: 0.8824, Train_f1: 0.6384, Val_f1: 0.7273\n",
      "Epoch: 040, Loss: 0.70641028881073, Train_Acc: 0.7720, Val_Acc: 0.8971, Train_f1: 0.6549, Val_f1: 0.7747\n",
      "Epoch: 041, Loss: 0.6654502153396606, Train_Acc: 0.7856, Val_Acc: 0.8971, Train_f1: 0.6824, Val_f1: 0.7747\n",
      "Epoch: 042, Loss: 0.6720505952835083, Train_Acc: 0.7946, Val_Acc: 0.8971, Train_f1: 0.7011, Val_f1: 0.7747\n",
      "Epoch: 043, Loss: 0.6314386129379272, Train_Acc: 0.8014, Val_Acc: 0.9118, Train_f1: 0.7140, Val_f1: 0.8303\n",
      "Epoch: 044, Loss: 0.6466740369796753, Train_Acc: 0.8059, Val_Acc: 0.9118, Train_f1: 0.7213, Val_f1: 0.8303\n",
      "Epoch: 045, Loss: 0.6242356896400452, Train_Acc: 0.8217, Val_Acc: 0.9118, Train_f1: 0.7550, Val_f1: 0.8303\n",
      "Epoch: 046, Loss: 0.6174407601356506, Train_Acc: 0.8262, Val_Acc: 0.9118, Train_f1: 0.7677, Val_f1: 0.8303\n",
      "Epoch: 047, Loss: 0.5966545939445496, Train_Acc: 0.8284, Val_Acc: 0.9118, Train_f1: 0.7694, Val_f1: 0.8303\n",
      "Epoch: 048, Loss: 0.5941317081451416, Train_Acc: 0.8375, Val_Acc: 0.9265, Train_f1: 0.7878, Val_f1: 0.8697\n",
      "Epoch: 049, Loss: 0.5928593873977661, Train_Acc: 0.8375, Val_Acc: 0.9265, Train_f1: 0.7872, Val_f1: 0.8697\n",
      "Epoch: 050, Loss: 0.566942572593689, Train_Acc: 0.8465, Val_Acc: 0.9118, Train_f1: 0.8006, Val_f1: 0.8547\n",
      "Epoch: 051, Loss: 0.5528494119644165, Train_Acc: 0.8488, Val_Acc: 0.9118, Train_f1: 0.8049, Val_f1: 0.8547\n",
      "Epoch: 052, Loss: 0.5389639139175415, Train_Acc: 0.8510, Val_Acc: 0.9118, Train_f1: 0.8096, Val_f1: 0.8547\n",
      "Epoch: 053, Loss: 0.5297672748565674, Train_Acc: 0.8578, Val_Acc: 0.9118, Train_f1: 0.8200, Val_f1: 0.8547\n",
      "Epoch: 054, Loss: 0.5273805856704712, Train_Acc: 0.8600, Val_Acc: 0.9118, Train_f1: 0.8245, Val_f1: 0.8547\n",
      "Epoch: 055, Loss: 0.5224650502204895, Train_Acc: 0.8646, Val_Acc: 0.9118, Train_f1: 0.8283, Val_f1: 0.8547\n",
      "Epoch: 056, Loss: 0.5149720311164856, Train_Acc: 0.8691, Val_Acc: 0.9118, Train_f1: 0.8367, Val_f1: 0.8547\n",
      "Epoch: 057, Loss: 0.4768160581588745, Train_Acc: 0.8713, Val_Acc: 0.9118, Train_f1: 0.8394, Val_f1: 0.8547\n",
      "Epoch: 058, Loss: 0.48495447635650635, Train_Acc: 0.8713, Val_Acc: 0.9118, Train_f1: 0.8394, Val_f1: 0.8547\n",
      "Epoch: 059, Loss: 0.4556809067726135, Train_Acc: 0.8713, Val_Acc: 0.9118, Train_f1: 0.8398, Val_f1: 0.8547\n",
      "Epoch: 060, Loss: 0.45402973890304565, Train_Acc: 0.8736, Val_Acc: 0.9118, Train_f1: 0.8425, Val_f1: 0.8547\n",
      "Epoch: 061, Loss: 0.467307984828949, Train_Acc: 0.8758, Val_Acc: 0.9118, Train_f1: 0.8451, Val_f1: 0.8547\n",
      "Epoch: 062, Loss: 0.46456781029701233, Train_Acc: 0.8758, Val_Acc: 0.9118, Train_f1: 0.8451, Val_f1: 0.8547\n",
      "Epoch: 063, Loss: 0.4545056223869324, Train_Acc: 0.8804, Val_Acc: 0.9118, Train_f1: 0.8532, Val_f1: 0.8547\n",
      "Epoch: 064, Loss: 0.4383823275566101, Train_Acc: 0.8826, Val_Acc: 0.9118, Train_f1: 0.8576, Val_f1: 0.8547\n",
      "Epoch: 065, Loss: 0.4307260513305664, Train_Acc: 0.8826, Val_Acc: 0.9118, Train_f1: 0.8576, Val_f1: 0.8547\n",
      "Epoch: 066, Loss: 0.4409571886062622, Train_Acc: 0.8804, Val_Acc: 0.9118, Train_f1: 0.8532, Val_f1: 0.8547\n",
      "Epoch: 067, Loss: 0.42642468214035034, Train_Acc: 0.8804, Val_Acc: 0.9118, Train_f1: 0.8532, Val_f1: 0.8547\n",
      "Epoch: 068, Loss: 0.41673561930656433, Train_Acc: 0.8871, Val_Acc: 0.9118, Train_f1: 0.8627, Val_f1: 0.8547\n",
      "Epoch: 069, Loss: 0.4109524190425873, Train_Acc: 0.8849, Val_Acc: 0.9118, Train_f1: 0.8609, Val_f1: 0.8547\n",
      "Epoch: 070, Loss: 0.3970828652381897, Train_Acc: 0.8826, Val_Acc: 0.9118, Train_f1: 0.8592, Val_f1: 0.8547\n",
      "Epoch: 071, Loss: 0.4077197015285492, Train_Acc: 0.8826, Val_Acc: 0.9118, Train_f1: 0.8592, Val_f1: 0.8547\n",
      "Epoch: 072, Loss: 0.38590699434280396, Train_Acc: 0.8826, Val_Acc: 0.9118, Train_f1: 0.8592, Val_f1: 0.8547\n",
      "Epoch: 073, Loss: 0.3866419196128845, Train_Acc: 0.8826, Val_Acc: 0.9118, Train_f1: 0.8592, Val_f1: 0.8547\n",
      "Epoch: 074, Loss: 0.3809935748577118, Train_Acc: 0.8804, Val_Acc: 0.9118, Train_f1: 0.8555, Val_f1: 0.8547\n",
      "Epoch: 075, Loss: 0.37738656997680664, Train_Acc: 0.8849, Val_Acc: 0.9118, Train_f1: 0.8616, Val_f1: 0.8547\n",
      "Epoch: 076, Loss: 0.3750559985637665, Train_Acc: 0.8826, Val_Acc: 0.9118, Train_f1: 0.8599, Val_f1: 0.8547\n",
      "Epoch: 077, Loss: 0.37598660588264465, Train_Acc: 0.8826, Val_Acc: 0.9118, Train_f1: 0.8599, Val_f1: 0.8547\n",
      "Epoch: 078, Loss: 0.37039443850517273, Train_Acc: 0.8849, Val_Acc: 0.9118, Train_f1: 0.8624, Val_f1: 0.8547\n",
      "Epoch: 079, Loss: 0.36239051818847656, Train_Acc: 0.8871, Val_Acc: 0.9265, Train_f1: 0.8668, Val_f1: 0.8867\n",
      "Epoch: 080, Loss: 0.3481586277484894, Train_Acc: 0.8871, Val_Acc: 0.9265, Train_f1: 0.8668, Val_f1: 0.8867\n",
      "Epoch: 081, Loss: 0.3340409994125366, Train_Acc: 0.8916, Val_Acc: 0.9265, Train_f1: 0.8736, Val_f1: 0.8867\n",
      "Epoch: 082, Loss: 0.34850165247917175, Train_Acc: 0.8916, Val_Acc: 0.9265, Train_f1: 0.8736, Val_f1: 0.8867\n",
      "Epoch: 083, Loss: 0.3465135395526886, Train_Acc: 0.8939, Val_Acc: 0.9265, Train_f1: 0.8776, Val_f1: 0.8867\n",
      "Epoch: 084, Loss: 0.3469145596027374, Train_Acc: 0.8962, Val_Acc: 0.9265, Train_f1: 0.8801, Val_f1: 0.8867\n",
      "Epoch: 085, Loss: 0.35164567828178406, Train_Acc: 0.8984, Val_Acc: 0.9265, Train_f1: 0.8826, Val_f1: 0.8867\n",
      "Epoch: 086, Loss: 0.31631070375442505, Train_Acc: 0.9052, Val_Acc: 0.9265, Train_f1: 0.8915, Val_f1: 0.8867\n",
      "Epoch: 087, Loss: 0.3450087904930115, Train_Acc: 0.9052, Val_Acc: 0.9265, Train_f1: 0.8915, Val_f1: 0.8867\n",
      "Epoch: 088, Loss: 0.34272435307502747, Train_Acc: 0.9097, Val_Acc: 0.9265, Train_f1: 0.8963, Val_f1: 0.8867\n",
      "Epoch: 089, Loss: 0.3278845250606537, Train_Acc: 0.9097, Val_Acc: 0.9265, Train_f1: 0.8963, Val_f1: 0.8867\n",
      "Epoch: 090, Loss: 0.3273226320743561, Train_Acc: 0.9074, Val_Acc: 0.9265, Train_f1: 0.8939, Val_f1: 0.8867\n",
      "Epoch: 091, Loss: 0.33536213636398315, Train_Acc: 0.9097, Val_Acc: 0.9265, Train_f1: 0.8963, Val_f1: 0.8867\n",
      "Epoch: 092, Loss: 0.332282155752182, Train_Acc: 0.9097, Val_Acc: 0.9265, Train_f1: 0.8963, Val_f1: 0.8867\n",
      "Epoch: 093, Loss: 0.3268788158893585, Train_Acc: 0.9142, Val_Acc: 0.9265, Train_f1: 0.9028, Val_f1: 0.8867\n",
      "Epoch: 094, Loss: 0.3090239465236664, Train_Acc: 0.9120, Val_Acc: 0.9412, Train_f1: 0.8989, Val_f1: 0.9147\n",
      "Epoch: 095, Loss: 0.3031046390533447, Train_Acc: 0.9120, Val_Acc: 0.9559, Train_f1: 0.8989, Val_f1: 0.9394\n",
      "Epoch: 096, Loss: 0.2999848425388336, Train_Acc: 0.9120, Val_Acc: 0.9412, Train_f1: 0.8989, Val_f1: 0.9147\n",
      "Epoch: 097, Loss: 0.29451075196266174, Train_Acc: 0.9120, Val_Acc: 0.9412, Train_f1: 0.8989, Val_f1: 0.9147\n",
      "Epoch: 098, Loss: 0.30276596546173096, Train_Acc: 0.9120, Val_Acc: 0.9412, Train_f1: 0.8989, Val_f1: 0.9147\n",
      "Epoch: 099, Loss: 0.30351758003234863, Train_Acc: 0.9142, Val_Acc: 0.9412, Train_f1: 0.9030, Val_f1: 0.9147\n",
      "Epoch: 100, Loss: 0.30495017766952515, Train_Acc: 0.9142, Val_Acc: 0.9412, Train_f1: 0.9030, Val_f1: 0.9147\n"
     ]
    }
   ],
   "source": [
    "model.train_loop(data, optimizer, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_classes, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.Sigmoid())\n",
    "\n",
    "        self.fc2 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.Sigmoid())\n",
    "        \n",
    "        self.fc_cls = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc_cls(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def train_loop(self, dataloader, optimizer, epochs=100):\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.train()\n",
    "\n",
    "            for batch_idx, (x, y) in enumerate(dataloader):\n",
    "                optimizer.zero_grad()\n",
    "                x = x.to(DEVICE)\n",
    "                pred = self.forward(x)\n",
    "                \n",
    "                loss = F.cross_entropy(pred, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # train_acc, train_f1 = self.validate(data, data.train_mask)\n",
    "                # val_acc, val_f1 = self.validate(data, data.test_mask)\n",
    "                print(loss)\n",
    "                # log(\n",
    "                #     Epoch=epoch,\n",
    "                #     Loss=loss,\n",
    "                #     Train_Acc=train_acc,\n",
    "                #     Val_Acc=val_acc,\n",
    "                #     Train_f1=train_f1,\n",
    "                #     Val_f1=val_f1,\n",
    "                # )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, data, mask):\n",
    "        self.eval()\n",
    "        pred = self.forward(data.x, data.edge_index, data.edge_attr)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "\n",
    "        f1 = f1_score(\n",
    "            data.y[mask].cpu().numpy(),\n",
    "            pred[mask].cpu().numpy(),\n",
    "            average=\"macro\",\n",
    "        )\n",
    "        acc = accuracy_score(data.y[mask].cpu().numpy(), pred[mask].cpu().numpy())\n",
    "\n",
    "        return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = torch.tensor(x, dtype=torch.float32)\n",
    "pam50 = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "input_dim = dataset.x.shape[1]\n",
    "hidden_dim = 64\n",
    "n_classes = len(labels)\n",
    "dropout = 0.5\n",
    "lr = 0.001\n",
    "wd = 0.01\n",
    "\n",
    "model = MLP(input_dim, hidden_dim, n_classes, dropout)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr, weight_decay=wd)\n",
    "\n",
    "model.to(DEVICE)\n",
    "data = dataset.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected 2D or 3D input (got 1D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[214], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpam50\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[212], line 114\u001b[0m, in \u001b[0;36mMLP.train_loop\u001b[0;34m(self, dataloader, optimizer, epochs)\u001b[0m\n\u001b[1;32m    112\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    113\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m--> 114\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(pred, y)\n\u001b[1;32m    117\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[212], line 97\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 97\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(x)\n\u001b[1;32m     99\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/miniconda3/envs/omics/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/omics/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/omics/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/omics/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_input_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/omics/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:301\u001b[0m, in \u001b[0;36mBatchNorm1d._check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_input_dim\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected 2D or 3D input (got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mD input)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim())\n\u001b[1;32m    303\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: expected 2D or 3D input (got 1D input)"
     ]
    }
   ],
   "source": [
    "model.train_loop(zip(latent,pam50), optimizer, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
