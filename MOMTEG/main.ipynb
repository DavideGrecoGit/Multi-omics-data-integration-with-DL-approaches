{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.logging import log\n",
    "from torch_geometric.utils import to_edge_index\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import random\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "\n",
    "def setup_seed(seed=SEED):\n",
    "    \"\"\"\n",
    "    setup seed to make the experiments deterministic\n",
    "\n",
    "    Parameters:\n",
    "        seed(int) -- the random seed\n",
    "\n",
    "    @source https://github.com/zhangxiaoyu11/OmiEmbed\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "setup_seed(42)\n",
    "\n",
    "def get_edge_index(X, threshold=0.005):\n",
    "    if threshold:\n",
    "        X[X < threshold] = 0\n",
    "\n",
    "    return to_edge_index((torch.tensor(X, dtype=torch.float).to_sparse()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latent = pd.read_csv(\"./data/MoGCN_results/latent_data.csv\")\n",
    "x = df_latent.iloc[:, 1:].values\n",
    "\n",
    "df_results = pd.read_csv(\"./data/TCGA/sample_classes.csv\")\n",
    "y = df_results[\"class\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LumA', 'LumB', 'Basal', 'Her2']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = df_results[[\"PAM50Call_RNAseq\",\"class\"]].value_counts()\n",
    "counts = counts.reset_index().sort_values([\"class\"])\n",
    "labels = counts[\"PAM50Call_RNAseq\"].to_list()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snf = pd.read_csv(\"./data/MoGCN_results/SNF_fused_matrix.csv\")\n",
    "adj = df_snf.iloc[:,1:].values\n",
    "np.fill_diagonal(adj, 0)\n",
    "edge_index, edge_attr = get_edge_index(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([866])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test = pd.read_csv(\"./data/TCGA/test_sample.csv\")\n",
    "test_mask = np.array(df_results[\"Sample_ID\"].isin(list_test[\"Sample_ID\"]))\n",
    "\n",
    "list_train = pd.read_csv(\"./data/TCGA/train_sample.csv\")\n",
    "train_mask = np.array(df_results[\"Sample_ID\"].isin(list_train[\"Sample_ID\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     A1-A0SF\n",
       "1     A1-A0SJ\n",
       "2     A1-A0SK\n",
       "3     A1-A0SO\n",
       "4     A1-A0SQ\n",
       "       ...   \n",
       "63    A7-A0DC\n",
       "64    A7-A13D\n",
       "65    A7-A13E\n",
       "66    A7-A26E\n",
       "67    A7-A26G\n",
       "Name: Sample_ID, Length: 68, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[\"Sample_ID\"][test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Data(\n",
    "    x=torch.tensor(x, dtype=torch.float32),\n",
    "    # edge_index = torch.tensor([[], []], dtype=torch.long),\n",
    "    edge_index=edge_index,\n",
    "    edge_attr=edge_attr,\n",
    "    y=torch.tensor(y, dtype=torch.long),\n",
    ")\n",
    "\n",
    "dataset.train_mask = train_mask\n",
    "dataset.test_mask = test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def train_loop(self, data, optimizer, epochs=100):\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred, _ = self.forward(data.x, data.edge_index, data.edge_attr)\n",
    "            loss = F.cross_entropy(pred[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_acc, train_f1 = self.validate(data, data.train_mask)\n",
    "            val_acc, val_f1 = self.validate(data, data.test_mask)\n",
    "\n",
    "            log(\n",
    "                Epoch=epoch,\n",
    "                Loss=loss,\n",
    "                Train_Acc=train_acc,\n",
    "                Val_Acc=val_acc,\n",
    "                Train_f1=train_f1,\n",
    "                Val_f1=val_f1,\n",
    "            )\n",
    "\n",
    "        return val_acc, val_f1\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, data, mask):\n",
    "        self.eval()\n",
    "        pred, _ = self.forward(data.x, data.edge_index, data.edge_attr)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "\n",
    "        f1 = f1_score(\n",
    "            data.y[mask].cpu().numpy(),\n",
    "            pred[mask].cpu().numpy(),\n",
    "            average=\"macro\",\n",
    "        )\n",
    "        acc = accuracy_score(data.y[mask].cpu().numpy(), pred[mask].cpu().numpy())\n",
    "\n",
    "        return acc, f1\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_latent_space(self, data, save_path=None):\n",
    "        self.eval()\n",
    "        pred, latent = self.forward(data.x, data.edge_index)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "\n",
    "        latent = latent.cpu().numpy()\n",
    "\n",
    "        if save_path:\n",
    "            np.savetxt(\n",
    "                os.path.join(save_path),\n",
    "                latent,\n",
    "                delimiter=\",\",\n",
    "            )\n",
    "\n",
    "        return latent\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_predictions(self, data, save_dir=None, mask=None):\n",
    "        self.eval()\n",
    "        pred, _ = self.forward(data.x, data.edge_index)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        pred = pred.cpu().numpy()\n",
    "\n",
    "        if mask is not None:\n",
    "            pred = pred[mask]\n",
    "\n",
    "        if save_dir is not None:\n",
    "            data_to_save = {\"GT\": data.y.cpu().numpy(), \"Pred\": pred}\n",
    "            df = pd.DataFrame.from_dict(data_to_save)\n",
    "            df.to_csv(os.path.join(save_dir, \"predictions.csv\"), index=False)\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class GCN(GNN):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.fc = nn.Linear(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = F.elu(x)\n",
    "        y = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        y = self.fc(y)\n",
    "        return y, x\n",
    "\n",
    "\n",
    "class GAT(GNN):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels)\n",
    "        self.conv2 = GATv2Conv(hidden_channels, hidden_channels)\n",
    "        self.fc = GATv2Conv(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        # x = self.fc(x)\n",
    "        x = self.fc(x, edge_index, edge_weight)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = dataset.x.shape[1]\n",
    "hidden_dim = 64\n",
    "n_classes = len(labels)\n",
    "dropout = 0.5\n",
    "lr = 0.001\n",
    "wd = 0.01\n",
    "\n",
    "model = GCN(input_dim, hidden_dim, n_classes, dropout)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr, weight_decay=wd)\n",
    "\n",
    "model.to(DEVICE)\n",
    "data = dataset.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.462212085723877, Train_Acc: 0.2483, Val_Acc: 0.1618, Train_f1: 0.1445, Val_f1: 0.1001\n",
      "Epoch: 002, Loss: 1.3743162155151367, Train_Acc: 0.4537, Val_Acc: 0.5147, Train_f1: 0.1908, Val_f1: 0.2102\n",
      "Epoch: 003, Loss: 1.3590714931488037, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 004, Loss: 1.3313932418823242, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 005, Loss: 1.3012152910232544, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 006, Loss: 1.2632569074630737, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 007, Loss: 1.258150339126587, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 008, Loss: 1.2332664728164673, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 009, Loss: 1.2133655548095703, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 010, Loss: 1.2296950817108154, Train_Acc: 0.4808, Val_Acc: 0.5147, Train_f1: 0.1623, Val_f1: 0.1699\n",
      "Epoch: 011, Loss: 1.202070951461792, Train_Acc: 0.4831, Val_Acc: 0.5147, Train_f1: 0.1681, Val_f1: 0.1699\n",
      "Epoch: 012, Loss: 1.175587773323059, Train_Acc: 0.4898, Val_Acc: 0.5294, Train_f1: 0.1846, Val_f1: 0.1933\n",
      "Epoch: 013, Loss: 1.1701058149337769, Train_Acc: 0.5124, Val_Acc: 0.5882, Train_f1: 0.2332, Val_f1: 0.2712\n",
      "Epoch: 014, Loss: 1.1507577896118164, Train_Acc: 0.5395, Val_Acc: 0.6471, Train_f1: 0.2811, Val_f1: 0.3313\n",
      "Epoch: 015, Loss: 1.1190671920776367, Train_Acc: 0.5756, Val_Acc: 0.6618, Train_f1: 0.3325, Val_f1: 0.3444\n",
      "Epoch: 016, Loss: 1.1350449323654175, Train_Acc: 0.5937, Val_Acc: 0.6912, Train_f1: 0.3543, Val_f1: 0.3688\n",
      "Epoch: 017, Loss: 1.100830316543579, Train_Acc: 0.6072, Val_Acc: 0.7059, Train_f1: 0.3693, Val_f1: 0.3802\n",
      "Epoch: 018, Loss: 1.1049209833145142, Train_Acc: 0.6230, Val_Acc: 0.7353, Train_f1: 0.3855, Val_f1: 0.4016\n",
      "Epoch: 019, Loss: 1.072372555732727, Train_Acc: 0.6321, Val_Acc: 0.7500, Train_f1: 0.3942, Val_f1: 0.4117\n",
      "Epoch: 020, Loss: 1.0570333003997803, Train_Acc: 0.6388, Val_Acc: 0.7500, Train_f1: 0.4005, Val_f1: 0.4117\n",
      "Epoch: 021, Loss: 1.0275248289108276, Train_Acc: 0.6524, Val_Acc: 0.7500, Train_f1: 0.4125, Val_f1: 0.4117\n",
      "Epoch: 022, Loss: 1.0314253568649292, Train_Acc: 0.6569, Val_Acc: 0.7500, Train_f1: 0.4164, Val_f1: 0.4117\n",
      "Epoch: 023, Loss: 0.9842175841331482, Train_Acc: 0.6614, Val_Acc: 0.7500, Train_f1: 0.4202, Val_f1: 0.4117\n",
      "Epoch: 024, Loss: 0.9866352081298828, Train_Acc: 0.6637, Val_Acc: 0.7500, Train_f1: 0.4200, Val_f1: 0.4117\n",
      "Epoch: 025, Loss: 0.9761464595794678, Train_Acc: 0.6682, Val_Acc: 0.7647, Train_f1: 0.4266, Val_f1: 0.4214\n",
      "Epoch: 026, Loss: 0.940032958984375, Train_Acc: 0.6727, Val_Acc: 0.7647, Train_f1: 0.4303, Val_f1: 0.4214\n",
      "Epoch: 027, Loss: 0.952852189540863, Train_Acc: 0.6772, Val_Acc: 0.7794, Train_f1: 0.4329, Val_f1: 0.4309\n",
      "Epoch: 028, Loss: 0.9197214841842651, Train_Acc: 0.6817, Val_Acc: 0.7941, Train_f1: 0.4354, Val_f1: 0.4400\n",
      "Epoch: 029, Loss: 0.9152040481567383, Train_Acc: 0.6840, Val_Acc: 0.8088, Train_f1: 0.4392, Val_f1: 0.4489\n",
      "Epoch: 030, Loss: 0.897662341594696, Train_Acc: 0.6885, Val_Acc: 0.8235, Train_f1: 0.4531, Val_f1: 0.4576\n",
      "Epoch: 031, Loss: 0.882895290851593, Train_Acc: 0.6907, Val_Acc: 0.8235, Train_f1: 0.4577, Val_f1: 0.4576\n",
      "Epoch: 032, Loss: 0.872735321521759, Train_Acc: 0.6930, Val_Acc: 0.8235, Train_f1: 0.4622, Val_f1: 0.4576\n",
      "Epoch: 033, Loss: 0.8376753926277161, Train_Acc: 0.7020, Val_Acc: 0.8235, Train_f1: 0.4885, Val_f1: 0.4576\n",
      "Epoch: 034, Loss: 0.8200956583023071, Train_Acc: 0.7111, Val_Acc: 0.8529, Train_f1: 0.5120, Val_f1: 0.5910\n",
      "Epoch: 035, Loss: 0.8269135355949402, Train_Acc: 0.7223, Val_Acc: 0.8529, Train_f1: 0.5522, Val_f1: 0.5885\n",
      "Epoch: 036, Loss: 0.7917444705963135, Train_Acc: 0.7359, Val_Acc: 0.8529, Train_f1: 0.5913, Val_f1: 0.5885\n",
      "Epoch: 037, Loss: 0.7840628623962402, Train_Acc: 0.7381, Val_Acc: 0.8529, Train_f1: 0.5974, Val_f1: 0.5885\n",
      "Epoch: 038, Loss: 0.7468101978302002, Train_Acc: 0.7427, Val_Acc: 0.8529, Train_f1: 0.6093, Val_f1: 0.5885\n",
      "Epoch: 039, Loss: 0.7433850169181824, Train_Acc: 0.7427, Val_Acc: 0.8676, Train_f1: 0.6085, Val_f1: 0.6688\n",
      "Epoch: 040, Loss: 0.7414160966873169, Train_Acc: 0.7494, Val_Acc: 0.8676, Train_f1: 0.6259, Val_f1: 0.6688\n",
      "Epoch: 041, Loss: 0.6940428614616394, Train_Acc: 0.7494, Val_Acc: 0.8676, Train_f1: 0.6259, Val_f1: 0.6688\n",
      "Epoch: 042, Loss: 0.7178254723548889, Train_Acc: 0.7494, Val_Acc: 0.8676, Train_f1: 0.6259, Val_f1: 0.6688\n",
      "Epoch: 043, Loss: 0.6849218606948853, Train_Acc: 0.7585, Val_Acc: 0.8676, Train_f1: 0.6458, Val_f1: 0.6688\n",
      "Epoch: 044, Loss: 0.665309727191925, Train_Acc: 0.7698, Val_Acc: 0.8676, Train_f1: 0.6695, Val_f1: 0.6688\n",
      "Epoch: 045, Loss: 0.6617445945739746, Train_Acc: 0.7788, Val_Acc: 0.8971, Train_f1: 0.6874, Val_f1: 0.7828\n",
      "Epoch: 046, Loss: 0.6406918168067932, Train_Acc: 0.7901, Val_Acc: 0.8971, Train_f1: 0.7108, Val_f1: 0.7828\n",
      "Epoch: 047, Loss: 0.6292205452919006, Train_Acc: 0.8036, Val_Acc: 0.9118, Train_f1: 0.7342, Val_f1: 0.8303\n",
      "Epoch: 048, Loss: 0.616210401058197, Train_Acc: 0.8104, Val_Acc: 0.9118, Train_f1: 0.7505, Val_f1: 0.8303\n",
      "Epoch: 049, Loss: 0.6097855567932129, Train_Acc: 0.8149, Val_Acc: 0.9118, Train_f1: 0.7592, Val_f1: 0.8303\n",
      "Epoch: 050, Loss: 0.5987327098846436, Train_Acc: 0.8149, Val_Acc: 0.9118, Train_f1: 0.7587, Val_f1: 0.8303\n",
      "Epoch: 051, Loss: 0.5927863121032715, Train_Acc: 0.8284, Val_Acc: 0.9118, Train_f1: 0.7801, Val_f1: 0.8303\n",
      "Epoch: 052, Loss: 0.5820158123970032, Train_Acc: 0.8352, Val_Acc: 0.9118, Train_f1: 0.7931, Val_f1: 0.8303\n",
      "Epoch: 053, Loss: 0.5597168803215027, Train_Acc: 0.8397, Val_Acc: 0.9118, Train_f1: 0.8011, Val_f1: 0.8303\n",
      "Epoch: 054, Loss: 0.5601546168327332, Train_Acc: 0.8488, Val_Acc: 0.9118, Train_f1: 0.8145, Val_f1: 0.8303\n",
      "Epoch: 055, Loss: 0.5432079434394836, Train_Acc: 0.8533, Val_Acc: 0.9118, Train_f1: 0.8219, Val_f1: 0.8303\n",
      "Epoch: 056, Loss: 0.5139455199241638, Train_Acc: 0.8578, Val_Acc: 0.9118, Train_f1: 0.8274, Val_f1: 0.8303\n",
      "Epoch: 057, Loss: 0.5254842638969421, Train_Acc: 0.8578, Val_Acc: 0.9118, Train_f1: 0.8273, Val_f1: 0.8303\n",
      "Epoch: 058, Loss: 0.5307732820510864, Train_Acc: 0.8578, Val_Acc: 0.9118, Train_f1: 0.8286, Val_f1: 0.8303\n",
      "Epoch: 059, Loss: 0.5033335089683533, Train_Acc: 0.8600, Val_Acc: 0.9118, Train_f1: 0.8316, Val_f1: 0.8303\n",
      "Epoch: 060, Loss: 0.4812653660774231, Train_Acc: 0.8600, Val_Acc: 0.9118, Train_f1: 0.8316, Val_f1: 0.8303\n",
      "Epoch: 061, Loss: 0.4779698848724365, Train_Acc: 0.8623, Val_Acc: 0.9118, Train_f1: 0.8352, Val_f1: 0.8303\n",
      "Epoch: 062, Loss: 0.49221271276474, Train_Acc: 0.8646, Val_Acc: 0.9118, Train_f1: 0.8382, Val_f1: 0.8303\n",
      "Epoch: 063, Loss: 0.48941364884376526, Train_Acc: 0.8623, Val_Acc: 0.9118, Train_f1: 0.8333, Val_f1: 0.8303\n",
      "Epoch: 064, Loss: 0.46256667375564575, Train_Acc: 0.8623, Val_Acc: 0.9118, Train_f1: 0.8333, Val_f1: 0.8303\n",
      "Epoch: 065, Loss: 0.48212310671806335, Train_Acc: 0.8646, Val_Acc: 0.9118, Train_f1: 0.8363, Val_f1: 0.8303\n",
      "Epoch: 066, Loss: 0.4544316828250885, Train_Acc: 0.8646, Val_Acc: 0.9118, Train_f1: 0.8338, Val_f1: 0.8303\n",
      "Epoch: 067, Loss: 0.4524233043193817, Train_Acc: 0.8691, Val_Acc: 0.9118, Train_f1: 0.8391, Val_f1: 0.8303\n",
      "Epoch: 068, Loss: 0.44942304491996765, Train_Acc: 0.8713, Val_Acc: 0.9118, Train_f1: 0.8433, Val_f1: 0.8303\n",
      "Epoch: 069, Loss: 0.4583503007888794, Train_Acc: 0.8736, Val_Acc: 0.9118, Train_f1: 0.8444, Val_f1: 0.8303\n",
      "Epoch: 070, Loss: 0.4371577501296997, Train_Acc: 0.8758, Val_Acc: 0.9118, Train_f1: 0.8473, Val_f1: 0.8303\n",
      "Epoch: 071, Loss: 0.43561747670173645, Train_Acc: 0.8804, Val_Acc: 0.9118, Train_f1: 0.8544, Val_f1: 0.8303\n",
      "Epoch: 072, Loss: 0.42163196206092834, Train_Acc: 0.8804, Val_Acc: 0.9265, Train_f1: 0.8568, Val_f1: 0.8697\n",
      "Epoch: 073, Loss: 0.4160299301147461, Train_Acc: 0.8826, Val_Acc: 0.9265, Train_f1: 0.8610, Val_f1: 0.8697\n",
      "Epoch: 074, Loss: 0.41345515847206116, Train_Acc: 0.8894, Val_Acc: 0.9118, Train_f1: 0.8715, Val_f1: 0.8303\n",
      "Epoch: 075, Loss: 0.4119388163089752, Train_Acc: 0.8894, Val_Acc: 0.9118, Train_f1: 0.8719, Val_f1: 0.8303\n",
      "Epoch: 076, Loss: 0.4123964011669159, Train_Acc: 0.8849, Val_Acc: 0.9118, Train_f1: 0.8662, Val_f1: 0.8303\n",
      "Epoch: 077, Loss: 0.40971988439559937, Train_Acc: 0.8849, Val_Acc: 0.9265, Train_f1: 0.8662, Val_f1: 0.8697\n",
      "Epoch: 078, Loss: 0.38163432478904724, Train_Acc: 0.8939, Val_Acc: 0.9412, Train_f1: 0.8796, Val_f1: 0.9032\n",
      "Epoch: 079, Loss: 0.3949618935585022, Train_Acc: 0.8962, Val_Acc: 0.9412, Train_f1: 0.8823, Val_f1: 0.9032\n",
      "Epoch: 080, Loss: 0.3978939354419708, Train_Acc: 0.9007, Val_Acc: 0.9412, Train_f1: 0.8877, Val_f1: 0.9032\n",
      "Epoch: 081, Loss: 0.36959436535835266, Train_Acc: 0.9007, Val_Acc: 0.9412, Train_f1: 0.8868, Val_f1: 0.9032\n",
      "Epoch: 082, Loss: 0.3699871599674225, Train_Acc: 0.9007, Val_Acc: 0.9412, Train_f1: 0.8868, Val_f1: 0.9032\n",
      "Epoch: 083, Loss: 0.37512099742889404, Train_Acc: 0.9052, Val_Acc: 0.9412, Train_f1: 0.8919, Val_f1: 0.9032\n",
      "Epoch: 084, Loss: 0.3809816539287567, Train_Acc: 0.9052, Val_Acc: 0.9412, Train_f1: 0.8905, Val_f1: 0.9032\n",
      "Epoch: 085, Loss: 0.36704280972480774, Train_Acc: 0.9074, Val_Acc: 0.9412, Train_f1: 0.8930, Val_f1: 0.9032\n",
      "Epoch: 086, Loss: 0.3611423373222351, Train_Acc: 0.9074, Val_Acc: 0.9412, Train_f1: 0.8947, Val_f1: 0.9032\n",
      "Epoch: 087, Loss: 0.3550034761428833, Train_Acc: 0.9052, Val_Acc: 0.9412, Train_f1: 0.8922, Val_f1: 0.9032\n",
      "Epoch: 088, Loss: 0.3458249866962433, Train_Acc: 0.8984, Val_Acc: 0.9412, Train_f1: 0.8845, Val_f1: 0.9032\n",
      "Epoch: 089, Loss: 0.36106204986572266, Train_Acc: 0.8939, Val_Acc: 0.9412, Train_f1: 0.8779, Val_f1: 0.9032\n",
      "Epoch: 090, Loss: 0.35526689887046814, Train_Acc: 0.8939, Val_Acc: 0.9412, Train_f1: 0.8779, Val_f1: 0.9032\n",
      "Epoch: 091, Loss: 0.3476668894290924, Train_Acc: 0.9074, Val_Acc: 0.9412, Train_f1: 0.8962, Val_f1: 0.9032\n",
      "Epoch: 092, Loss: 0.3533598780632019, Train_Acc: 0.9120, Val_Acc: 0.9412, Train_f1: 0.9012, Val_f1: 0.9032\n",
      "Epoch: 093, Loss: 0.3329484760761261, Train_Acc: 0.9142, Val_Acc: 0.9412, Train_f1: 0.9037, Val_f1: 0.9032\n",
      "Epoch: 094, Loss: 0.33786284923553467, Train_Acc: 0.9120, Val_Acc: 0.9412, Train_f1: 0.9025, Val_f1: 0.9032\n",
      "Epoch: 095, Loss: 0.33068332076072693, Train_Acc: 0.9142, Val_Acc: 0.9412, Train_f1: 0.9038, Val_f1: 0.9032\n",
      "Epoch: 096, Loss: 0.31928735971450806, Train_Acc: 0.9142, Val_Acc: 0.9412, Train_f1: 0.9038, Val_f1: 0.9032\n",
      "Epoch: 097, Loss: 0.3434748649597168, Train_Acc: 0.9142, Val_Acc: 0.9412, Train_f1: 0.9038, Val_f1: 0.9032\n",
      "Epoch: 098, Loss: 0.3238479793071747, Train_Acc: 0.9187, Val_Acc: 0.9412, Train_f1: 0.9096, Val_f1: 0.9032\n",
      "Epoch: 099, Loss: 0.3441821336746216, Train_Acc: 0.9165, Val_Acc: 0.9412, Train_f1: 0.9066, Val_f1: 0.9032\n",
      "Epoch: 100, Loss: 0.32971498370170593, Train_Acc: 0.9187, Val_Acc: 0.9412, Train_f1: 0.9084, Val_f1: 0.9032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9411764705882353, 0.9031531531531531)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_loop(data, optimizer, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square: 15.549027844748213\n",
      "P-value: 0.001402794029028031\n",
      "       counts  observed   expected  statistic\n",
      "group                                        \n",
      "0         269        33  40.770375  -7.770375\n",
      "1          73        17   8.865199   8.134801\n",
      "2         109        15  20.887802  -5.887802\n",
      "3          52        12   6.476624   5.523376\n"
     ]
    }
   ],
   "source": [
    "from sksurv.compare import compare_survival\n",
    "\n",
    "def print_chisq(surv_data, groups):\n",
    "    data_y = surv_data[[\"Status\",\"Survival_in_days\"]].to_records(index=False)\n",
    "    chisq, pvalue, stats, covar = compare_survival(data_y, groups, return_stats=True)\n",
    "\n",
    "    print(f\"Chi-square: {chisq}\")\n",
    "    print(f\"P-value: {pvalue}\")\n",
    "    print(stats)\n",
    "\n",
    "surv_data = pd.read_csv(\"./data/TCGA/survival_data.csv\")\n",
    "pred = model.get_predictions(data)\n",
    "mask = np.array(df_results[\"Sample_ID\"].isin(surv_data[\"Sample_ID\"]))\n",
    "print_chisq(surv_data,pred[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_classes, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.Sigmoid())\n",
    "\n",
    "        self.fc2 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.Sigmoid())\n",
    "        \n",
    "        self.fc_cls = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc_cls(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def train_loop(self, dataloader, optimizer, epochs=100):\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.train()\n",
    "\n",
    "            for batch_idx, (x, y) in enumerate(dataloader):\n",
    "                optimizer.zero_grad()\n",
    "                x = x.to(DEVICE)\n",
    "                pred = self.forward(x)\n",
    "                \n",
    "                loss = F.cross_entropy(pred, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # train_acc, train_f1 = self.validate(data, data.train_mask)\n",
    "                # val_acc, val_f1 = self.validate(data, data.test_mask)\n",
    "                print(loss)\n",
    "                # log(\n",
    "                #     Epoch=epoch,\n",
    "                #     Loss=loss,\n",
    "                #     Train_Acc=train_acc,\n",
    "                #     Val_Acc=val_acc,\n",
    "                #     Train_f1=train_f1,\n",
    "                #     Val_f1=val_f1,\n",
    "                # )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, data, mask):\n",
    "        self.eval()\n",
    "        pred = self.forward(data.x, data.edge_index, data.edge_attr)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "\n",
    "        f1 = f1_score(\n",
    "            data.y[mask].cpu().numpy(),\n",
    "            pred[mask].cpu().numpy(),\n",
    "            average=\"macro\",\n",
    "        )\n",
    "        acc = accuracy_score(data.y[mask].cpu().numpy(), pred[mask].cpu().numpy())\n",
    "\n",
    "        return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = torch.tensor(x, dtype=torch.float32)\n",
    "pam50 = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "input_dim = dataset.x.shape[1]\n",
    "hidden_dim = 64\n",
    "n_classes = len(labels)\n",
    "dropout = 0.5\n",
    "lr = 0.001\n",
    "wd = 0.01\n",
    "\n",
    "model = MLP(input_dim, hidden_dim, n_classes, dropout)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr, weight_decay=wd)\n",
    "\n",
    "model.to(DEVICE)\n",
    "data = dataset.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected 2D or 3D input (got 1D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[214], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpam50\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[212], line 114\u001b[0m, in \u001b[0;36mMLP.train_loop\u001b[0;34m(self, dataloader, optimizer, epochs)\u001b[0m\n\u001b[1;32m    112\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    113\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m--> 114\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(pred, y)\n\u001b[1;32m    117\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[212], line 97\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 97\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(x)\n\u001b[1;32m     99\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/miniconda3/envs/omics/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/omics/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/omics/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/omics/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_input_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/omics/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:301\u001b[0m, in \u001b[0;36mBatchNorm1d._check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_input_dim\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected 2D or 3D input (got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mD input)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim())\n\u001b[1;32m    303\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: expected 2D or 3D input (got 1D input)"
     ]
    }
   ],
   "source": [
    "model.train_loop(zip(latent,pam50), optimizer, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
